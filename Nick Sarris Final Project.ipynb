{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA model for Tesla Stock\n",
    "\n",
    "Nick Sarris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    "- http://people.duke.edu/~rnau/Notes_on_nonseasonal_ARIMA_models--Robert_Nau.pdf\n",
    "    - https://people.duke.edu/~rnau/arimrule.htm\n",
    "\n",
    "- http://www.michaeljgrogan.com/arima-model-statsmodels-python/\n",
    "\n",
    "- https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "#ggplot makes fun looking graphs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [12.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print \"Current size:\", fig_size\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File Documents/TSLA_10Y(2).csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9b44a51cd550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Import Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mTSLA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Documents/TSLA_10Y(2).csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTSLA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nick1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nick1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nick1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nick1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nick1\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File Documents/TSLA_10Y(2).csv does not exist"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "\n",
    "TSLA = pd.read_csv('Documents/TSLA_10Y(2).csv')\n",
    "\n",
    "TSLA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSLA.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting index to datetime object\n",
    "\n",
    "TSLA.index = pd.to_datetime(TSLA.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Open = TSLA['Open']\n",
    "High = TSLA['High']\n",
    "Low = TSLA['Low']\n",
    "Close = TSLA['Close']\n",
    "Volume = TSLA['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we inspect the data to see if we can eye any trends with fast python commands and visualizations. \n",
    "    We start by looking at every column of our Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas rolling method used \n",
    "\n",
    "Open.rolling(30).mean().plot(label = '30 day rolling mean')\n",
    "Open.rolling(365).mean().plot(label = 'Yearly rolling mean')\n",
    "Open.rolling(30).std().plot(label = '30 day rolling std')\n",
    "Open.rolling(365).std().plot(label = 'Yearly rolling std')\n",
    "plt.plot(Open, alpha = .70, label = 'Open')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Open Price of Tesla Stock (10Y)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High.rolling(30).mean().plot(label = '30 day rolling mean')\n",
    "High.rolling(365).mean().plot(label = 'Yearly rolling mean')\n",
    "High.rolling(30).std().plot(label = '30 day rolling std')\n",
    "High.rolling(365).std().plot(label = 'Yearly rolling std')\n",
    "plt.plot(High, alpha = .70, label = 'Low')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('High Price of Tesla Stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Low.rolling(30).mean().plot(label = '30 day rolling mean')\n",
    "Low.rolling(365).mean().plot(label = 'Yearly rolling mean')\n",
    "Low.rolling(30).std().plot(label = '30 day rolling std')\n",
    "Low.rolling(365).std().plot(label = 'Yearly rolling std')\n",
    "plt.plot(Low, alpha = .70, label = 'Low')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Low Price of Tesla Stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Close.rolling(30).mean().plot(label = '30 day rolling mean')\n",
    "Close.rolling(365).mean().plot(label = 'Yearly rolling mean')\n",
    "Close.rolling(30).std().plot(label = '30 day rolling std')\n",
    "Close.rolling(365).std().plot(label = 'Yearly rolling std')\n",
    "plt.plot(Close, alpha = .70, label = 'Open')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Close Price of Tesla Stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volume.rolling(30).mean().plot(label = '30 day rolling mean')\n",
    "Volume.rolling(365).mean().plot(label = 'Yearly rolling mean')\n",
    "Volume.rolling(30).std().plot(label = '30 day rolling std')\n",
    "Volume.rolling(365).std().plot(label = 'Yearly rolling std')\n",
    "plt.plot(Volume, alpha = .70, label = 'Volume')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Volume Price of Tesla Stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs we can start to notice important information for the ARIMA parameter decision process: the data is not stationary. That is:\n",
    "\n",
    "- The mean changes with respect to time\n",
    "- The variance changes with respect to time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Arima, we would like for the mean and variance to stay constant. In order to smooth out the variance, we take the natural log of each column and run the statistical tests from above again to see if it was successful. We omit the Volume graph because the behavior of it's variance is more volatile than the others, thus making it a poor candidate for ARIMA modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen = np.log(Open)\n",
    "lnHigh = np.log(High)\n",
    "lnLow = np.log(Low)\n",
    "lnClose = np.log(Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the plot is shown to visualize each natural logged column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen.plot(label = 'ln(Open)', alpha = 0.5)\n",
    "lnHigh.plot(label = 'ln(High)', alpha = 0.5)\n",
    "lnLow.plot(label = 'ln(Low)', alpha = 0.5)\n",
    "lnClose.plot(label = 'ln(Close)', alpha = 0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with the graph below, it looks the same except the spikes aren't as big as the ones below, indicating we did smooth the variance. Looking at the rolling variances and means for the exponentially smoothed graphs, as was done above, would give definitive proof of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSLA.Open.plot(alpha = .50, label = 'Open')\n",
    "TSLA.High.plot(alpha = .50, label = 'High')\n",
    "TSLA.Low.plot(alpha = .50, label = 'Low')\n",
    "TSLA.Close.plot(alpha = .50, label = 'Close')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lnOpen.rolling(30).var().plot(label = 'ln(Open)', alpha = 0.4)\n",
    "lnHigh.rolling(30).var().plot(label = 'ln(High)', alpha = 0.4)\n",
    "lnLow.rolling(30).var().plot(label = 'ln(Low)', alpha = 0.4)\n",
    "lnClose.rolling(30).var().plot(label = 'ln(Close)', alpha = 0.4)\n",
    "\n",
    "plt.hlines(lnOpen.rolling(30).var().mean(),\n",
    "          lnOpen.rolling(30).var().index[0],\n",
    "          lnOpen.rolling(30).var().index[-1], label = 'Mean')\n",
    "\n",
    "\n",
    "plt.title('Monthly rolling variance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen.rolling(30).var().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen.rolling(30).var().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen.rolling(30).mean().plot(label = 'ln(Open)', alpha = 0.4)\n",
    "lnHigh.rolling(30).mean().plot(label = 'ln(High)', alpha = 0.4)\n",
    "lnLow.rolling(30).mean().plot(label = 'ln(Low)', alpha = 0.4)\n",
    "lnClose.rolling(30).mean().plot(label = 'ln(Close)', alpha = 0.4)\n",
    "\n",
    "plt.hlines(lnOpen.rolling(30).mean().mean(),\n",
    "          lnOpen.rolling(30).mean().index[0],\n",
    "          lnOpen.rolling(30).mean().index[-1], label = 'Mean')\n",
    "\n",
    "\n",
    "plt.title('Monthly rolling mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen.rolling(30).mean().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important things should be noted:\n",
    "\n",
    "- After smoothing the data, we notice that both graphs exhibit a sharp increase in the data between 2013 and 2014\n",
    "- Relative to 10 years, all dataframe features in both graphs looks similar to one another\n",
    "    - This implies that in our model, we can choose only one feauture column to make forecasts on future prices \n",
    "        of the stock. This is important, since we cannot use cross-sectional data in a univariate ARIMA process.\n",
    "        \n",
    "\n",
    "We would like to assume a linear relationship with the data, so we want to exlude the left portion of graph that contains the spike and the lower prices. Since there is still around four years of data after the exlusion, the sample size is still large enough to continue analyzing. \n",
    "\n",
    "Following from the second bullet point, from now on we will be only using the Open feature column. It is chosen because it is less volatile than the High and Low prices, making its variance more stationary than the Open. However, all three columns could have been used and the model would predict similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the date of the spike\n",
    "\n",
    "lnOpen_sorted = lnOpen.rolling(30).var()\n",
    "\n",
    "lnOpen_sorted.sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding date where spike ended\n",
    "\n",
    "lnOpen_sorted['2013-05-31':'2013-06-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "#Plotting remaining 30 day rolling variance from above graph, excluding aformetioned spike and data before it\n",
    "\n",
    "lnOpen_var = lnOpen.rolling(30).var()['2014-06-25':'2018-12-03']\n",
    "\n",
    "lnOpen_var.plot(label = '30 day rolling variance')\n",
    "plt.hlines(lnOpen.rolling(30).var().mean(),\n",
    "          lnOpen.rolling(30).var().index[890],\n",
    "          lnOpen.rolling(30).var().index[-1], label = 'Unadjusted Mean', color = 'black')\n",
    "\n",
    "plt.hlines(lnOpen['2014-06-25':'2018-12-03'].rolling(30).var().mean(),\n",
    "          lnOpen.rolling(30).mean().index[890],\n",
    "          lnOpen.rolling(30).mean().index[-1], label = 'Adjusted Mean', color = 'green')\n",
    "\n",
    "plt.title('Open stock price, 30 day rolling variance, ADJ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting remaining 30 day rolling mean from above graph, excluding aformetioned spike and data before it\n",
    "\n",
    "lnOpen_mean = lnOpen.rolling(30).mean()['2014-06-25':'2018-12-03']\n",
    "\n",
    "lnOpen_mean.plot(label = '30 day rolling mean')\n",
    "\n",
    "plt.hlines(lnOpen.rolling(30).mean().mean(),\n",
    "          lnOpen.rolling(30).mean().index[890],\n",
    "          lnOpen.rolling(30).mean().index[-1], label = 'Unadjusted Mean', color = 'black')\n",
    "\n",
    "plt.hlines(lnOpen['2014-06-25':'2018-12-03'].rolling(30).mean().mean(),\n",
    "          lnOpen.rolling(30).mean().index[890],\n",
    "          lnOpen.rolling(30).mean().index[-1], label = 'Adjusted Mean', color = 'green')\n",
    "\n",
    "plt.title('Open stock price, 30 day rolling mean, ADJ')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting remaining lnOpen graph from above, excluding aformetioned spike and data before it\n",
    "\n",
    "lnOpen_ = lnOpen['2014-06-25':'2018-12-03']\n",
    "\n",
    "lnOpen_.plot()\n",
    "plt.title('Open stock price ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished pre-processing and smoothing the data, we can make it stationary. \n",
    "\n",
    "Pandas has a method for this called diff(), which takes entry in Dataframe[x+1] and subtracts entry Dataframe[x] from it for every x in the length of the index of the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen_.diff().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen_.diff().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the variance is approximately constant and the mean is approximately constant. \n",
    "\n",
    "**This indicates that the value d in ARIMA(p,d,q) is d = 1, since we only differenced our data once.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further test if our assumption that our data is stationary, we emply the Augmented Dicky Fuller Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adfuller is the Augmented Dicky Fuller test, which tests for stationarity\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnOpen__ = lnOpen_.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The important value is the second entry, which is called the p-value. The data is stationary if p < 0.05. Here, it is not.\n",
    "\n",
    "adfuller(lnOpen_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here p has been rounded down which is why it has value 0.0, so p < 0.05 and thus the data is stationary after differencing.\n",
    "\n",
    "adfuller(lnOpen__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding p,q for ARIMA(p,d,q) using ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.tsa.stattools as ts\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF, or AutoCorrelation Function, tests the correlation between the first instance and the first lag, the first lag and the second lag, the second lag and the third lag, etc. \n",
    "\n",
    "The PACF, or Partial AutoCorrelation Function, tests the correlation between the first instance and the first lag, the first instance and the second lag, the first instance and the third lag, etc. \n",
    "\n",
    "Interpreting what each graph shows will indicate what p and q value we need for ARIMA(p,d,q) (the p in ARIMA(p,d,q) has nothing to do the with the p from the adfuller test above).\n",
    "\n",
    "The p is associated with the AR model, or AutoRegressive model, which can be determined from the ACF. \n",
    "\n",
    "The q is associated with the MA mode, or the Moving Average model, which can be determined from the PACF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis is that the stock price of today is most likely correlated with the stock price of yesterday, and not correlated with the stock price of last week, or last month. \n",
    "\n",
    "Thus, we are expecting that our data will follow the behavior of the ACF, and therefore be an AR model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_1 =  acf(lnOpen_)[1:]\n",
    "\n",
    "test_df = pd.DataFrame([acf_1]).T\n",
    "test_df.columns = ['Pandas Autocorrelation']\n",
    "test_df.index += 1\n",
    "test_df.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear relationship shown here supports the hypothesis. Each lag is significantly correlated with the one before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_1 =  pacf(lnOpen_)[1:]\n",
    "#plt.plot(pacf_1)\n",
    "#plt.show()\n",
    "\n",
    "test_df = pd.DataFrame([pacf_1]).T\n",
    "test_df.columns = ['Pandas Partial Autocorrelation']\n",
    "test_df.index += 1\n",
    "test_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the PACF support the hypothesis. There is high correlation between the first instance and the first lag, but little to no correlation exists between the first instance the the kth lag for k > 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From both graphs, we can conclude that our hypothesis was correct and the data has AutoRegressive behavior. Thus, our model is said to be an AR(1) model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_matrix=lnOpen__.as_matrix()\n",
    "model = ARIMA(price_matrix, order=(1,1,0))\n",
    "model_fit = model.fit(disp=0, trend = 'c')\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows a summary of the results. Though I do not have the mathematical background to completely describe what everything in the summary means, the Real value is the coefficient of the AutoRegressive(1) equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "residuals.plot(kind='kde')\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows the residual data remaining after extracting the patterns in the ACF and the PACF. The method used to get the residual data is predictably the model.fit().resid. \n",
    "\n",
    "Since data represents white noise is and is approximately distributed, we know that all patterns were extracted no other patterns can be found in the data. If trends still existed in the residuals, one would have to go back and change their p or q value until the shape in both graphs above are achieved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code below takes 2/3 of the data for training and the remaining 1/3 for testing. We iterate through every date and make a prediction, then compare it to the actual data. \n",
    "\n",
    "Since we took the natural log earlier to smooth the variance of the data out, we will now undo it with the np.exp() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.exp(lnOpen_).values\n",
    "size = int(len(X) * 2/3)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(1,1,0))\n",
    "    model_fit = model.fit(trend = 'c')    \n",
    "    #predictions starts at index 1112 and ends at index 1119 (index 1119 is last entry), so\n",
    "    #a weeks worth of prices of the stock\n",
    "    output = model_fit.predict(1112, 1119, typ = 'levels')\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the analysis, confidence intervals will be plotted with the predictions graph in order to make better estimates for the forecast. The for-loop below finds each confidence interval for the last seven days in the dataframe, which are the indexes [362-373] (index 373 being the last entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions__ = pd.DataFrame(predictions)\n",
    "\n",
    "for i in range(7):\n",
    "    \n",
    "    n, min_max, mean, var, skew, kurt = stats.describe(predictions__[362+i:])\n",
    "    std= np.sqrt(var)\n",
    "    \n",
    "    Q = []\n",
    "    R = []\n",
    "    S = []\n",
    "    \n",
    "    # Confidence intervals of 25%,66%, and 95%\n",
    "    A = stats.norm.interval(0.95,loc=mean,scale=std)\n",
    "    B = stats.norm.interval(0.66,loc=mean,scale=std)\n",
    "    C = stats.norm.interval(0.25,loc=mean,scale=std)\n",
    "    \n",
    "    Q += C\n",
    "    R += B\n",
    "    S += A\n",
    "    \n",
    "    print(Q)\n",
    "    print(R)\n",
    "    print(S)\n",
    "    print('\\n')\n",
    "    #prints out [(min), (max)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the graph of the predictions with the confidence intervals. Of the 1119 entries in the original data, 1/3 of the data was for training, which is 373. The x-axis below accounts for all 373 testing points below (excluding the first 300, which was sliced out of the code). The y-axis represents the price. The MSE is approximately 101, which is a good outcome for a first time ARIMA. Though the index is not in Datetime, the index should be the last 7 days of the data (2018/11/26 - 2018/12/03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The floats in the plt.hlines are from the first set of Q,R,S lists from the for-loop above, since that index is when the ARIMA predictions started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[357:], color = 'black', label = 'rolling forecast prediction')\n",
    "plt.plot(test[357:], color = 'purple', label = 'actual result')\n",
    "\n",
    "plt.hlines(345.02695391, 9, 17, label = '25% Upper Conf Inv', color = 'red', alpha = 0.7)\n",
    "plt.hlines(347.55748155, 9, 17, label = '66% Upper Conf Inv', color = 'orange', alpha = 0.7)\n",
    "plt.hlines(351.56235615, 9, 17, label = '95% Upper Conf Inv', color = 'green', alpha = 0.7)\n",
    "\n",
    "plt.hlines(342.48944684, 9, 17, label = '25% Lower Conf Inv', color = 'red', alpha = 0.7)\n",
    "plt.hlines(339.9589192, 9, 17, label = '66% Lower Conf Inv', color = 'orange', alpha = 0.7)\n",
    "plt.hlines(335.9540446, 9, 17, label = '95% Lower Conf Inv', color = 'green', alpha = 0.7)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation is that the confidence intervals are not centered around the black prediction line. This could suggest that over the course of the week, the price is expected to grow more than it is going to decrease. Since the actual value towards the end increased, it could support this conclusion.\n",
    "\n",
    "Also, the spikes that dip below and above the confidence intervals show inaccuracies in the model. This suggests that the way stock prices behave cannot be captured solely through the ARIMA model. Perhaps there are other models and python libraries that can offer a better explanation for the erratic behavior such as *tweepy*, that conducts a sentiment analysis based off tweets. \n",
    "\n",
    "A lot of assumptions were made:\n",
    "\n",
    "- The data was stationary\n",
    "- No outside data influenced increases and decreases in stock price\n",
    "- A linear relationship between time and the price of the stock was assumed\n",
    "\n",
    "Though the model appears to be accurate, too many assumptions were made, and thus the bias is high. \n",
    "\n",
    "In the future, for more accuracy one should make auxiliary models and combine them with the outcome of this model for higher accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
